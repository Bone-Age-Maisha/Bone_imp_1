{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYx74TTLPisfZfQ01vZ9uA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bone-Age-Maisha/Bone_imp_1/blob/main/only_ch_attention_in_atlas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuetHNxiSVXU",
        "outputId": "d5a2cc93-9043-4c57-b6fe-d990501090f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi\n"
          ]
        }
      ],
      "source": [
        "print(\"hi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ME1eES_XVgf",
        "outputId": "5f9a74e7-3e04-4861-83c1-4c8333e8fabd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0pBLh9XyXkXw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mcUYEnuQabz5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_dir = '/content/drive/MyDrive/hand_atlas/image'\n",
        "df = pd.read_csv('/content/drive/MyDrive/hand_atlas/data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nWXSUCivakS0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "tf.config.experimental.set_memory_growth(gpus[0], True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-rrGBSsalk8",
        "outputId": "da33ae68-2473-47ab-8ffc-6e662dca4c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data set...\n"
          ]
        }
      ],
      "source": [
        "X_train = []\n",
        "y_age = []\n",
        "y_gender = []\n",
        "\n",
        "#df = pd.read_csv('/raid/chenchao/code/BoneAge/BoneAge/data/Training.csv')\n",
        "a = df.values\n",
        "m = a.shape[0]\n",
        "\n",
        "path = train_dir\n",
        "k = 0\n",
        "print ('Loading data set...')\n",
        "k=1\n",
        "for i in os.listdir(path):\n",
        "  #print(i)\n",
        "  if(i==\"3131.jpg\" or i==\"4520.jpg\" or i==\"3187.jpg\"):\n",
        "    continue\n",
        "  y_age.append(df.boneage[df.id == int(i[:-4])].tolist()[0])\n",
        "  y_gender.append(df.gender[df.id == int(i[:-4])].tolist()[0])\n",
        "  #print(i)\n",
        "  img_path = path + \"/\"+i\n",
        "  img = cv2.imread(img_path)\n",
        "  #print(img.shape)\n",
        "  #print (img_path)\n",
        "  img = cv2.imread(img_path)\n",
        "    #print (img_path)\n",
        "    #if(img is not None):\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  img = cv2.resize(img,(300,300))\n",
        "  x = np.asarray(img, dtype=np.uint8)\n",
        "  X_train.append(x)\n",
        "  k=k+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eONvDJ3zawit",
        "outputId": "f340db6d-84cf-4c42-8233-1a861a8d9d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: visualization in /usr/local/lib/python3.8/dist-packages (1.0.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (from visualization) (2.9.0)\n",
            "Requirement already satisfied: autolab-core in /usr/local/lib/python3.8/dist-packages (from visualization) (1.1.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from visualization) (3.2.2)\n",
            "Requirement already satisfied: pyrender in /usr/local/lib/python3.8/dist-packages (from visualization) (0.1.45)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from visualization) (1.21.6)\n",
            "Requirement already satisfied: trimesh[easy] in /usr/local/lib/python3.8/dist-packages (from visualization) (3.17.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from autolab-core->visualization) (4.6.0.66)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from autolab-core->visualization) (1.3.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from autolab-core->visualization) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from autolab-core->visualization) (1.2.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from autolab-core->visualization) (1.7.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (from autolab-core->visualization) (0.18.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from autolab-core->visualization) (1.0.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from autolab-core->visualization) (0.70.14)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.8/dist-packages (from autolab-core->visualization) (6.7.0)\n",
            "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.8/dist-packages (from autolab-core->visualization) (0.17.21)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->visualization) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->visualization) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->visualization) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->visualization) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->visualization) (1.15.0)\n",
            "Requirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.8/dist-packages (from multiprocess->autolab-core->visualization) (0.3.6)\n",
            "Requirement already satisfied: freetype-py in /usr/local/lib/python3.8/dist-packages (from pyrender->visualization) (2.3.0)\n",
            "Requirement already satisfied: PyOpenGL==3.1.0 in /usr/local/lib/python3.8/dist-packages (from pyrender->visualization) (3.1.0)\n",
            "Requirement already satisfied: pyglet>=1.4.10 in /usr/local/lib/python3.8/dist-packages (from pyrender->visualization) (1.5.27)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from pyrender->visualization) (2.6.3)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in /usr/local/lib/python3.8/dist-packages (from ruamel.yaml->autolab-core->visualization) (0.2.7)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image->autolab-core->visualization) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->autolab-core->visualization) (1.3.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->autolab-core->visualization) (3.1.0)\n",
            "Requirement already satisfied: mapbox-earcut in /usr/local/lib/python3.8/dist-packages (from trimesh[easy]->visualization) (1.0.1)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.8/dist-packages (from trimesh[easy]->visualization) (1.8.5.post1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from trimesh[easy]->visualization) (3.1.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from trimesh[easy]->visualization) (4.9.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.8/dist-packages (from trimesh[easy]->visualization) (3.0.4)\n",
            "Requirement already satisfied: rtree in /usr/local/lib/python3.8/dist-packages (from trimesh[easy]->visualization) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from trimesh[easy]->visualization) (2.23.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.8/dist-packages (from trimesh[easy]->visualization) (4.3.3)\n",
            "Requirement already satisfied: svg.path in /usr/local/lib/python3.8/dist-packages (from trimesh[easy]->visualization) (6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from trimesh[easy]->visualization) (57.4.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from trimesh[easy]->visualization) (1.7.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.8/dist-packages (from trimesh[easy]->visualization) (1.0.4)\n",
            "Requirement already satisfied: pycollada in /usr/local/lib/python3.8/dist-packages (from trimesh[easy]->visualization) (0.7.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->trimesh[easy]->visualization) (5.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->trimesh[easy]->visualization) (0.19.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->trimesh[easy]->visualization) (22.1.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema->trimesh[easy]->visualization) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->trimesh[easy]->visualization) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->trimesh[easy]->visualization) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->trimesh[easy]->visualization) (2.10)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->trimesh[easy]->visualization) (1.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BM3DQKSibZku"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from keras import backend as K\n",
        "from keras.preprocessing import image\n",
        "import os\n",
        "\n",
        "def softlabel(label,num_class):\n",
        "    softlabel=np.zeros((len(label),num_class))\n",
        "    ratio = 1.0/50\n",
        "    for i in range(len(label)):\n",
        "        for j in range(num_class):\n",
        "            softlabel[i,j]=1.0 - ratio*np.abs(j-label[i])\n",
        "    softlabel = np.maximum(softlabel,0)\n",
        "    return softlabel\n",
        "\n",
        "\n",
        "def ShowAttentionV1(model,image_path):\n",
        "    file_list = os.listdir(image_path)\n",
        "    file_list.sort()\n",
        "    for filename in file_list:\n",
        "        print (filename)\n",
        "        filepath=image_path+filename\n",
        "        image=load_image(filepath)\n",
        "        image = image/255.0\n",
        "        gender=1.0\n",
        "        gender=np.asarray(gender)\n",
        "        gender=np.expand_dims(gender,axis=0)\n",
        "        layer=K.function([model.layers[0].input],[model.layers[196].output])\n",
        "        FeatureMap=layer([image,gender])[0]\n",
        "        print (FeatureMap.shape)\n",
        "        FeatureMap = np.squeeze(FeatureMap, axis=0)\n",
        "        FeatureMap = np.abs(FeatureMap)\n",
        "        heatmap = np.mean(FeatureMap,axis=2)\n",
        "        heatmap = heatmap/np.max(heatmap)\n",
        "        heatmap = np.uint8(255*heatmap)\n",
        "        print (heatmap.shape)\n",
        "        heatmap = cv2.applyColorMap(heatmap,cv2.COLORMAP_JET)\n",
        "        SaveImg(filename,filepath,heatmap)\n",
        "    print ('********** Done ***********')\n",
        "\n",
        "def GAPAttention(model,weights,image_path):\n",
        "    file_list = os.listdir(train_dir)\n",
        "    file_list.sort()\n",
        "    for filename in file_list:\n",
        "        filepath=image_path+filename\n",
        "        print (filepath)\n",
        "        image=load_image(filepath)\n",
        "        print(\"gpa\")\n",
        "        print(image.shape)\n",
        "        image = image/255.0\n",
        "        gender=1.0\n",
        "        gender=np.asarray(gender)\n",
        "        gender=np.expand_dims(gender,axis=0)\n",
        "        print(\"ok\")\n",
        "        layer=K.function([model.layers[0].input],[model.layers[1].get_output_at(-1),model.layers[-1].output])\n",
        "        print(\"ok_1\")\n",
        "        GAP,prediction=layer([image])\n",
        "        print(\"ok_2\")\n",
        "        GAP=np.squeeze(GAP,axis=0)\n",
        "        print(\"ok_3\")\n",
        "        print (GAP.shape)\n",
        "        print(\"ok_4\")\n",
        "        index = np.argmax(prediction)\n",
        "        print(\"ok_5\")\n",
        "        print (index)\n",
        "       # weight = weights[:,index]\n",
        "        weight =np.mean(weights[:,index-5:index+5],axis=1)\n",
        "        heatmap = np.zeros((GAP.shape[0],GAP.shape[1]))\n",
        "        for k in range(GAP.shape[2]):\n",
        "            heatmap = heatmap + weight[k]*GAP[:,:,k]\n",
        "        heatmap = heatmap/np.max(heatmap)\n",
        "        heatmap = np.uint8(255*heatmap)\n",
        "        print (heatmap.shape)\n",
        "        heatmap = cv2.applyColorMap(heatmap,cv2.COLORMAP_JET)\n",
        "        SaveImg(filename,filepath,heatmap)\n",
        "    print ('********** Done ***********')\n",
        "\n",
        "def SaveImg(filename,filepath,heatmap):\n",
        "    img = cv2.imread(filepath)\n",
        "    heatmap = cv2.resize(heatmap,(img.shape[1],img.shape[0]))\n",
        "    AttentionImg =0.5* heatmap + img\n",
        "    cv2.imwrite('/content/heat'+filename,heatmap)\n",
        "    cv2.imwrite('/content/attention'+filename,AttentionImg)\n",
        "\n",
        "\n",
        "def load_image(path):\n",
        "    img = cv2.imread(path)\n",
        "    print(img.shape)\n",
        "    img = cv2.resize(img,(300,300))\n",
        "    print(img.shape)\n",
        "    x = np.asarray(img, dtype=np.float32)\n",
        "   # img = image.load_img(path, target_size=(448, 448))\n",
        "   # print (img.shape)\n",
        "   # x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    return x\n",
        "\n",
        "def TestMAE(model,test_data,test_label,test_gender):\n",
        "    test_gender = np.array(test_gender)\n",
        "    test_gender = np.expand_dims(test_gender,axis=1)\n",
        "    layer=K.function([model.layers[0].input,model.layers[3].input],[model.layers[-1].output])\n",
        "    predictions=layer([test_data,test_gender])\n",
        "    predictions = np.array(predictions)\n",
        "    predictions = np.squeeze(predictions,axis=0)\n",
        "    print (predictions.shape)\n",
        "    predict_label = np.argmax(predictions,axis=1)\n",
        "    test_label = np.argmax(test_label,axis=1)\n",
        "    print (predict_label)\n",
        "    print (test_label)\n",
        "    TestMAE = np.mean(np.abs(predict_label-test_label))\n",
        "    return TestMAE\n",
        "    \n",
        "def DataAugment(x_train):\n",
        "    x_train_Aug = np.zeros(x_train.shape)\n",
        "    for i in range(x_train.shape[0]):\n",
        "        for j in range(3):\n",
        "            img = x_train[i,:,:,j]\n",
        "            img = RandomMask(img)\n",
        "            img = RandomMask(img)\n",
        "            if np.random.random()>-1:\n",
        "                x_train_Aug[i,:,:,j]=img \n",
        "            else:\n",
        "                x_train_Aug[i,:,:,j]=x_train[i,:,:,j]\n",
        "    return x_train_Aug\n",
        "\n",
        "\n",
        "def RandomMask(img):\n",
        "    m,n=img.shape\n",
        "    m=int(m/6)\n",
        "    n=int(n/6)\n",
        "    i,j = np.random.randint(0,6,2)\n",
        "    img[i*m:(i+1)*m,j*n:(j+1)*n]=np.random.random()\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyB_gYSNbjjP",
        "outputId": "cc774c10-26fe-4550-ac65-a894b82685f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 24.2  135.7  173.4   77.13 212.8  157.5   47.87  94.82 140.2   70.29\n",
            " 123.5  127.4   96.   170.2  174.5   81.86  65.59 141.2  164.9   22.29\n",
            "  90.12  57.24 169.6  145.9  101.1   70.32 102.6  197.5  141.8  176.5\n",
            " 162.   161.4  159.5  180.6   99.78 173.6  159.7  101.6  135.5  177.6\n",
            " 212.9  204.6  221.3   13.15 128.8  191.    72.36  91.86 158.4  222.8 ]\n",
            "[-1.  1.  1. -1. -1.  1.  1.  1. -1.  1. -1. -1.  1. -1. -1.  1. -1.  1.\n",
            "  1.  1. -1. -1. -1. -1. -1. -1. -1.  1.  1. -1.  1.  1.  1. -1. -1.  1.\n",
            "  1. -1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1.]\n",
            "x_train shape:(1286, 300, 300, 3)\n",
            "y_train shape:(1286,)\n",
            "gender_train shape:(1286,)\n",
            "x_valid shape:(50, 300, 300, 3)\n",
            "y_valid shape:(50,)\n",
            "gender_valid shape:(50,)\n",
            "x_test shape:(50, 300, 300, 3)\n",
            "y_test shape:(50,)\n"
          ]
        }
      ],
      "source": [
        "y = np.asarray(y_age)\n",
        "gender = np.asarray(y_gender)\n",
        "x=np.asarray(X_train, dtype=np.float32)\n",
        "x=x/255\n",
        "gender =2*( gender-0.5)\n",
        "x_final = []\n",
        "y_final = []\n",
        "gender_final = []\n",
        "\n",
        "# Shuffle images and split into train, validation and test sets\n",
        "#random_no = np.random.choice(x.shape[0], size=x.shape[0], replace=False)\n",
        "random_no = np.arange(x.shape[0])\n",
        "#print(random_no)\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(random_no)\n",
        "for i in random_no:\n",
        "    x_final.append(x[i,:,:,:])\n",
        "    y_final.append(y[i])\n",
        "    gender_final.append(gender[i])\n",
        "\n",
        "x_final = np.asarray(x_final)\n",
        "y_final = np.asarray(y_final)\n",
        "gender_final = np.asarray(gender_final)\n",
        "print (y_final[:50])\n",
        "print (gender_final[:50])\n",
        "k = 50 # Decides split count\n",
        "x_test = x_final[:k,:,:,:]\n",
        "y_test = y_final[:k]\n",
        "gender_test = gender_final[:k]\n",
        "x_valid = x_final[k:2*k,:,:,:]\n",
        "y_valid = y_final[k:2*k]\n",
        "gender_valid = gender_final[k:2*k]\n",
        "x_train = x_final[2*k:,:,:,:]\n",
        "y_train = y_final[2*k:]\n",
        "gender_train = gender_final[2*k:]\n",
        "\n",
        "## \n",
        "#y_test = keras.utils.to_categorical(y_test,240)\n",
        "#y_train = keras.utils.to_categorical(y_train,240)\n",
        "#y_valid = keras.utils.to_categorical(y_valid,240)\n",
        "\n",
        "\n",
        "\n",
        "print ('x_train shape:'+ str(x_train.shape))\n",
        "print ('y_train shape:'+ str(y_train.shape))\n",
        "print ('gender_train shape:'+ str(gender_train.shape))\n",
        "print ('x_valid shape:'+ str(x_valid.shape))\n",
        "print ('y_valid shape:'+ str(y_valid.shape))\n",
        "print ('gender_valid shape:' + str(gender_valid.shape))\n",
        "print ('x_test shape:'+ str(x_test.shape))\n",
        "print ('y_test shape:'+ str(y_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aG3BxPtKNfiZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.xception import Xception\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Flatten, Dense, Input, Reshape, Lambda, Multiply,Conv2D\n",
        "from keras import backend as K\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "\n",
        "batch_size = 16\n",
        "epochs = 30\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xA7Uz93QJBhM"
      },
      "outputs": [],
      "source": [
        "def channel_attention(input_feature):\n",
        "  kernel_initializer = tf.keras.initializers.variance_scaling()\n",
        "  bias_initializer = tf.constant_initializer(value=0.0)\n",
        "  channel = input_feature.get_shape()[-1]\n",
        "  ratio=8\n",
        "  avg_pool = tf.reduce_mean(input_feature, axis=[1,2], keepdims=True)\n",
        "  assert avg_pool.get_shape()[1:] == (1,1,channel)\n",
        "  avg_pool =Dense(\n",
        "                                 units=channel//ratio,\n",
        "                                 activation=tf.nn.relu,\n",
        "                                 kernel_initializer=kernel_initializer,\n",
        "                                 bias_initializer=bias_initializer) (avg_pool)\n",
        "  assert avg_pool.get_shape()[1:] == (1,1,channel//ratio)\n",
        "  avg_pool = Dense(\n",
        "                                 units=channel,                             \n",
        "                                 kernel_initializer=kernel_initializer,\n",
        "                                 bias_initializer=bias_initializer)  (avg_pool)  \n",
        "  assert avg_pool.get_shape()[1:] == (1,1,channel)\n",
        "\n",
        "  max_pool = tf.reduce_max(input_feature, axis=[1,2], keepdims=True)    \n",
        "  assert max_pool.get_shape()[1:] == (1,1,channel)\n",
        "  max_pool = Dense(\n",
        "                                 units=channel//ratio,\n",
        "                                 activation=tf.nn.relu) (max_pool)  \n",
        "  assert max_pool.get_shape()[1:] == (1,1,channel//ratio)\n",
        "  max_pool = Dense(\n",
        "                                 units=channel)  (max_pool)\n",
        "  assert max_pool.get_shape()[1:] == (1,1,channel)\n",
        "\n",
        "  scale = tf.keras.activations.sigmoid(avg_pool + max_pool) \n",
        "  return input_feature * scale\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8ZavY92NJi21"
      },
      "outputs": [],
      "source": [
        "def spatial_attention(input_feature):\n",
        "  kernel_size = 7\n",
        "  kernel_initializer = tf.keras.initializers.variance_scaling()\n",
        "\n",
        "  avg_pool = tf.reduce_mean(input_feature, axis=[3], keepdims=True)\n",
        "  assert avg_pool.get_shape()[-1] == 1\n",
        "  max_pool = tf.reduce_max(input_feature, axis=[3], keepdims=True)\n",
        "  assert max_pool.get_shape()[-1] == 1\n",
        "  concat = tf.concat([avg_pool,max_pool], 3)\n",
        "  assert concat.get_shape()[-1] == 2\n",
        "\n",
        "  concat = Conv2D(\n",
        "                              filters=1,\n",
        "                              kernel_size=[kernel_size,kernel_size],\n",
        "                              strides=[1,1],\n",
        "                              padding=\"same\",\n",
        "                              activation=None,\n",
        "                              kernel_initializer=kernel_initializer,\n",
        "                              use_bias=False)(concat)\n",
        "  assert concat.get_shape()[-1] == 1\n",
        "  concat = tf.keras.activations.sigmoid(concat)\n",
        "    \n",
        "  return input_feature * concat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjQHE8fMJrKs",
        "outputId": "44c9f6fe-8154-4953-e2bc-a171c4a9f215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 8, 8, 2048)\n",
            "(None, 8, 8, 2048)\n",
            "(None, 8, 8, 2048)\n",
            "(None, 2, 2, 2048)\n",
            "(None, 8208)\n",
            "0 input1\n",
            "1 inception_v3\n",
            "2 tf.math.reduce_mean\n",
            "3 tf.math.reduce_max\n",
            "4 dense_1\n",
            "5 dense_3\n",
            "6 dense_2\n",
            "7 dense_4\n",
            "8 tf.__operators__.add\n",
            "9 tf.math.sigmoid\n",
            "10 tf.math.multiply\n",
            "11 max_pooling2d_4\n",
            "12 input2\n",
            "13 flatten\n",
            "14 dense\n",
            "15 concatenate_2\n",
            "16 dense_5\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input1 (InputLayer)            [(None, 300, 300, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " inception_v3 (Functional)      (None, None, None,   21802784    ['input1[0][0]']                 \n",
            "                                2048)                                                             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpLambd  (None, 1, 1, 2048)  0           ['inception_v3[0][0]']           \n",
            " a)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.reduce_max (TFOpLambda  (None, 1, 1, 2048)  0           ['inception_v3[0][0]']           \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1, 1, 256)    524544      ['tf.math.reduce_mean[0][0]']    \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1, 1, 256)    524544      ['tf.math.reduce_max[0][0]']     \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1, 1, 2048)   526336      ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 1, 1, 2048)   526336      ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 1, 1, 2048)  0           ['dense_2[0][0]',                \n",
            " da)                                                              'dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.sigmoid (TFOpLambda)   (None, 1, 1, 2048)   0           ['tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " tf.math.multiply (TFOpLambda)  (None, 8, 8, 2048)   0           ['inception_v3[0][0]',           \n",
            "                                                                  'tf.math.sigmoid[0][0]']        \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPooling2D)  (None, 2, 2, 2048)  0           ['tf.math.multiply[0][0]']       \n",
            "                                                                                                  \n",
            " input2 (InputLayer)            [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 8192)         0           ['max_pooling2d_4[0][0]']        \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 16)           32          ['input2[0][0]']                 \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 8208)         0           ['flatten[0][0]',                \n",
            "                                                                  'dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 1)            8209        ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,912,785\n",
            "Trainable params: 23,878,353\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "322/322 [==============================] - 47s 96ms/step - loss: 30.0119 - MAE: 30.0119 - val_loss: 28.0655 - val_MAE: 28.0655\n",
            "Epoch 2/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 20.2108 - MAE: 20.2108 - val_loss: 19.5198 - val_MAE: 19.5198\n",
            "Epoch 3/30\n",
            "322/322 [==============================] - 27s 83ms/step - loss: 16.8994 - MAE: 16.8994 - val_loss: 22.6363 - val_MAE: 22.6363\n",
            "Epoch 4/30\n",
            "322/322 [==============================] - 27s 83ms/step - loss: 14.7385 - MAE: 14.7385 - val_loss: 15.5091 - val_MAE: 15.5091\n",
            "Epoch 5/30\n",
            "322/322 [==============================] - 27s 83ms/step - loss: 13.7877 - MAE: 13.7877 - val_loss: 18.1395 - val_MAE: 18.1395\n",
            "Epoch 6/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 12.5974 - MAE: 12.5974 - val_loss: 22.4468 - val_MAE: 22.4468\n",
            "Epoch 7/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 12.2759 - MAE: 12.2759 - val_loss: 23.2788 - val_MAE: 23.2788\n",
            "Epoch 8/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 11.5133 - MAE: 11.5133 - val_loss: 20.3168 - val_MAE: 20.3168\n",
            "Epoch 9/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 11.0663 - MAE: 11.0663 - val_loss: 16.8373 - val_MAE: 16.8373\n",
            "Epoch 10/30\n",
            "322/322 [==============================] - 27s 83ms/step - loss: 9.7034 - MAE: 9.7034 - val_loss: 16.9444 - val_MAE: 16.9444\n",
            "Epoch 11/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 9.7549 - MAE: 9.7549 - val_loss: 14.5957 - val_MAE: 14.5957\n",
            "Epoch 12/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 9.7509 - MAE: 9.7509 - val_loss: 15.8309 - val_MAE: 15.8309\n",
            "Epoch 13/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 9.3235 - MAE: 9.3235 - val_loss: 14.7901 - val_MAE: 14.7901\n",
            "Epoch 14/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 8.7363 - MAE: 8.7363 - val_loss: 15.9639 - val_MAE: 15.9639\n",
            "Epoch 15/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 8.5972 - MAE: 8.5972 - val_loss: 14.4313 - val_MAE: 14.4313\n",
            "Epoch 16/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 8.3496 - MAE: 8.3496 - val_loss: 16.9817 - val_MAE: 16.9817\n",
            "Epoch 17/30\n",
            "322/322 [==============================] - 27s 83ms/step - loss: 8.0340 - MAE: 8.0340 - val_loss: 14.7224 - val_MAE: 14.7224\n",
            "Epoch 18/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 7.5483 - MAE: 7.5483 - val_loss: 14.3797 - val_MAE: 14.3797\n",
            "Epoch 19/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 7.4921 - MAE: 7.4921 - val_loss: 13.0862 - val_MAE: 13.0862\n",
            "Epoch 20/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 6.8957 - MAE: 6.8957 - val_loss: 14.2706 - val_MAE: 14.2706\n",
            "Epoch 21/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 6.6325 - MAE: 6.6325 - val_loss: 14.2036 - val_MAE: 14.2036\n",
            "Epoch 22/30\n",
            "322/322 [==============================] - 27s 83ms/step - loss: 6.6066 - MAE: 6.6066 - val_loss: 15.1164 - val_MAE: 15.1164\n",
            "Epoch 23/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 6.6270 - MAE: 6.6270 - val_loss: 13.0644 - val_MAE: 13.0644\n",
            "Epoch 24/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 6.3820 - MAE: 6.3820 - val_loss: 14.2286 - val_MAE: 14.2286\n",
            "Epoch 25/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 6.0455 - MAE: 6.0455 - val_loss: 13.5006 - val_MAE: 13.5006\n",
            "Epoch 26/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 5.9868 - MAE: 5.9868 - val_loss: 16.0420 - val_MAE: 16.0420\n",
            "Epoch 27/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 5.5243 - MAE: 5.5243 - val_loss: 14.2725 - val_MAE: 14.2725\n",
            "Epoch 28/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 5.3872 - MAE: 5.3872 - val_loss: 13.5925 - val_MAE: 13.5925\n",
            "Epoch 29/30\n",
            "322/322 [==============================] - 26s 82ms/step - loss: 5.6382 - MAE: 5.6382 - val_loss: 13.9244 - val_MAE: 13.9244\n",
            "Epoch 30/30\n",
            "322/322 [==============================] - 27s 84ms/step - loss: 5.4068 - MAE: 5.4068 - val_loss: 14.8403 - val_MAE: 14.8403\n",
            "4/4 [==============================] - 2s 53ms/step - loss: 12.1561 - MAE: 12.1561\n",
            "Test loss: 12.15605354309082\n",
            "Test MAE: 12.15605354309082\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
        "input = Input(shape=(300,300,3),name='input1')\n",
        "input_gender = Input(shape=(1,),dtype='float32',name='input2')\n",
        "output = base_model(input)\n",
        "gender_embedding=Dense(16)(input_gender)\n",
        "print (K.int_shape(output))\n",
        "x=channel_attention(output)\n",
        "print (K.int_shape(x))\n",
        "#x=spatial_attention(x)\n",
        "print (K.int_shape(x))\n",
        "x = keras.layers.MaxPooling2D(pool_size=(4,4))(x)\n",
        "print (K.int_shape(x))\n",
        "x=Flatten()(x)\n",
        "f= keras.layers.Concatenate(axis=1)([x,gender_embedding])\n",
        "print (K.int_shape(f)) \n",
        "#x = Dense(256, activation='relu')(x)\n",
        "prediction = Dense(1)(f)\n",
        "\n",
        "model = Model(inputs=[input,input_gender], outputs=prediction)\n",
        "for i,layer in enumerate(model.layers):\n",
        "    print (i,layer.name)\n",
        "\n",
        "Adam=tf.keras.optimizers.Adam(lr=0.0003,beta_1=0.9,beta_2=0.999)\n",
        "model.compile(optimizer=Adam, loss='mean_absolute_error', metrics=['MAE'])\n",
        "model.summary()\n",
        "\n",
        "checkpoint =keras.callbacks.ModelCheckpoint(filepath='weights_invcam_atlas.{epoch:02d}-{val_loss:.2f}.hdf5',save_weights_only=True,period=30)\n",
        "#model.fit_generator(DataGen.flow([x_train,gender_train],y_train,batch_size=batch_size),steps_per_epoch=np.ceil(len(y_train)/batch_size),epochs=50,verbose=1,validation_data=([x_valid,gender_valid,agev],y_valid))\n",
        "history=model.fit([x_train,gender_train],y_train,batch_size=4,epochs=30,verbose=1,validation_data=([x_valid,gender_valid],y_valid),callbacks = [checkpoint])\n",
        "score = model.evaluate([x_test,gender_test], y_test, batch_size=batch_size)\n",
        "print('Test loss:', score[0])\n",
        "print('Test MAE:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-Zt97SCwJrsN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "65b5e9dc-0cd6-4770-9e6d-501958fba7d5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "k3kEaXwRKYxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2a481c4-6a1c-4e27-c9f8-8236c41c5418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 8, 8, 1536)\n",
            "(None, 8, 8, 1536)\n",
            "(None, 2, 2, 1536)\n",
            "(None, 6160)\n",
            "0 input1\n",
            "1 inception_resnet_v2\n",
            "2 tf.math.reduce_mean_1\n",
            "3 tf.math.reduce_max_1\n",
            "4 dense_7\n",
            "5 dense_9\n",
            "6 dense_8\n",
            "7 dense_10\n",
            "8 tf.__operators__.add_1\n",
            "9 tf.math.sigmoid_1\n",
            "10 tf.math.multiply_1\n",
            "11 max_pooling2d_9\n",
            "12 input2\n",
            "13 flatten_1\n",
            "14 dense_6\n",
            "15 concatenate_3\n",
            "16 dense_11\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input1 (InputLayer)            [(None, 300, 300, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " inception_resnet_v2 (Functiona  (None, None, None,   54336736   ['input1[0][0]']                 \n",
            " l)                             1536)                                                             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_1 (TFOpLam  (None, 1, 1, 1536)  0           ['inception_resnet_v2[0][0]']    \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.math.reduce_max_1 (TFOpLamb  (None, 1, 1, 1536)  0           ['inception_resnet_v2[0][0]']    \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 1, 1, 192)    295104      ['tf.math.reduce_mean_1[0][0]']  \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 1, 1, 192)    295104      ['tf.math.reduce_max_1[0][0]']   \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 1, 1, 1536)   296448      ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 1, 1, 1536)   296448      ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 1, 1, 1536)  0           ['dense_8[0][0]',                \n",
            " mbda)                                                            'dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_1 (TFOpLambda)  (None, 1, 1, 1536)  0           ['tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " tf.math.multiply_1 (TFOpLambda  (None, 8, 8, 1536)  0           ['inception_resnet_v2[0][0]',    \n",
            " )                                                                'tf.math.sigmoid_1[0][0]']      \n",
            "                                                                                                  \n",
            " max_pooling2d_9 (MaxPooling2D)  (None, 2, 2, 1536)  0           ['tf.math.multiply_1[0][0]']     \n",
            "                                                                                                  \n",
            " input2 (InputLayer)            [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 6144)         0           ['max_pooling2d_9[0][0]']        \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 16)           32          ['input2[0][0]']                 \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 6160)         0           ['flatten_1[0][0]',              \n",
            "                                                                  'dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 1)            6161        ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 55,526,033\n",
            "Trainable params: 55,465,489\n",
            "Non-trainable params: 60,544\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "322/322 [==============================] - 86s 208ms/step - loss: 27.9743 - MAE: 27.9743 - val_loss: 20.0075 - val_MAE: 20.0075\n",
            "Epoch 2/30\n",
            "322/322 [==============================] - 62s 192ms/step - loss: 17.0637 - MAE: 17.0637 - val_loss: 16.3816 - val_MAE: 16.3816\n",
            "Epoch 3/30\n",
            "322/322 [==============================] - 62s 191ms/step - loss: 14.4074 - MAE: 14.4074 - val_loss: 18.6651 - val_MAE: 18.6651\n",
            "Epoch 4/30\n",
            "322/322 [==============================] - 62s 191ms/step - loss: 12.9386 - MAE: 12.9386 - val_loss: 18.4156 - val_MAE: 18.4156\n",
            "Epoch 5/30\n",
            "322/322 [==============================] - 62s 192ms/step - loss: 11.4217 - MAE: 11.4217 - val_loss: 15.3814 - val_MAE: 15.3814\n",
            "Epoch 6/30\n",
            "322/322 [==============================] - 62s 192ms/step - loss: 10.5772 - MAE: 10.5772 - val_loss: 16.8175 - val_MAE: 16.8175\n",
            "Epoch 7/30\n",
            "322/322 [==============================] - 61s 191ms/step - loss: 10.2008 - MAE: 10.2008 - val_loss: 15.2065 - val_MAE: 15.2065\n",
            "Epoch 8/30\n",
            "322/322 [==============================] - 61s 190ms/step - loss: 9.0086 - MAE: 9.0086 - val_loss: 13.8729 - val_MAE: 13.8729\n",
            "Epoch 9/30\n",
            "322/322 [==============================] - 61s 190ms/step - loss: 8.7801 - MAE: 8.7801 - val_loss: 17.5176 - val_MAE: 17.5176\n",
            "Epoch 10/30\n",
            "322/322 [==============================] - 61s 190ms/step - loss: 7.7911 - MAE: 7.7911 - val_loss: 13.8775 - val_MAE: 13.8775\n",
            "Epoch 11/30\n",
            "322/322 [==============================] - 61s 191ms/step - loss: 7.4476 - MAE: 7.4476 - val_loss: 17.2256 - val_MAE: 17.2256\n",
            "Epoch 12/30\n",
            "322/322 [==============================] - 61s 190ms/step - loss: 7.4304 - MAE: 7.4304 - val_loss: 13.5384 - val_MAE: 13.5384\n",
            "Epoch 13/30\n",
            "322/322 [==============================] - 61s 190ms/step - loss: 6.9657 - MAE: 6.9657 - val_loss: 14.8012 - val_MAE: 14.8012\n",
            "Epoch 14/30\n",
            "322/322 [==============================] - 61s 190ms/step - loss: 6.8083 - MAE: 6.8083 - val_loss: 12.6567 - val_MAE: 12.6567\n",
            "Epoch 15/30\n",
            "322/322 [==============================] - 61s 190ms/step - loss: 6.5170 - MAE: 6.5170 - val_loss: 14.5916 - val_MAE: 14.5916\n",
            "Epoch 16/30\n",
            "322/322 [==============================] - 62s 191ms/step - loss: 6.2126 - MAE: 6.2126 - val_loss: 12.9461 - val_MAE: 12.9461\n",
            "Epoch 17/30\n",
            "322/322 [==============================] - 61s 190ms/step - loss: 6.1059 - MAE: 6.1059 - val_loss: 13.8981 - val_MAE: 13.8981\n",
            "Epoch 18/30\n",
            "322/322 [==============================] - 62s 192ms/step - loss: 6.1595 - MAE: 6.1595 - val_loss: 14.4389 - val_MAE: 14.4389\n",
            "Epoch 19/30\n",
            "322/322 [==============================] - 61s 190ms/step - loss: 6.1251 - MAE: 6.1251 - val_loss: 12.9256 - val_MAE: 12.9256\n",
            "Epoch 20/30\n",
            "322/322 [==============================] - 62s 192ms/step - loss: 5.4571 - MAE: 5.4571 - val_loss: 14.9411 - val_MAE: 14.9411\n",
            "Epoch 21/30\n",
            "322/322 [==============================] - 61s 190ms/step - loss: 5.4124 - MAE: 5.4124 - val_loss: 13.1058 - val_MAE: 13.1058\n",
            "Epoch 22/30\n",
            "322/322 [==============================] - 61s 191ms/step - loss: 5.5121 - MAE: 5.5121 - val_loss: 12.2595 - val_MAE: 12.2595\n",
            "Epoch 23/30\n",
            "322/322 [==============================] - 61s 190ms/step - loss: 5.2084 - MAE: 5.2084 - val_loss: 12.8498 - val_MAE: 12.8498\n",
            "Epoch 24/30\n",
            "322/322 [==============================] - 61s 190ms/step - loss: 5.6122 - MAE: 5.6122 - val_loss: 13.1381 - val_MAE: 13.1381\n",
            "Epoch 25/30\n",
            "322/322 [==============================] - 61s 190ms/step - loss: 5.1001 - MAE: 5.1001 - val_loss: 12.6693 - val_MAE: 12.6693\n",
            "Epoch 26/30\n",
            "322/322 [==============================] - 61s 190ms/step - loss: 4.9031 - MAE: 4.9031 - val_loss: 13.3996 - val_MAE: 13.3996\n",
            "Epoch 27/30\n",
            "322/322 [==============================] - 61s 190ms/step - loss: 5.0295 - MAE: 5.0295 - val_loss: 13.4484 - val_MAE: 13.4484\n",
            "Epoch 28/30\n",
            "322/322 [==============================] - 61s 190ms/step - loss: 4.5157 - MAE: 4.5157 - val_loss: 12.8545 - val_MAE: 12.8545\n",
            "Epoch 29/30\n",
            "322/322 [==============================] - 61s 190ms/step - loss: 4.5490 - MAE: 4.5490 - val_loss: 12.6404 - val_MAE: 12.6404\n",
            "Epoch 30/30\n",
            "322/322 [==============================] - 62s 193ms/step - loss: 4.9197 - MAE: 4.9197 - val_loss: 12.6874 - val_MAE: 12.6874\n",
            "4/4 [==============================] - 2s 126ms/step - loss: 11.0471 - MAE: 11.0471\n",
            "Test loss: 11.047133445739746\n",
            "Test MAE: 11.047133445739746\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "\n",
        "base_model = InceptionResNetV2(weights='imagenet', include_top=False)\n",
        "input = Input(shape=(300,300,3),name='input1')\n",
        "input_gender = Input(shape=(1,),dtype='float32',name='input2')\n",
        "output = base_model(input)\n",
        "gender_embedding=Dense(16)(input_gender)\n",
        "print (K.int_shape(output))\n",
        "x=channel_attention(output)\n",
        "#print (K.int_shape(x))\n",
        "#x=spatial_attention(x)\n",
        "print (K.int_shape(x))\n",
        "x = keras.layers.MaxPooling2D(pool_size=(4,4))(x)\n",
        "print (K.int_shape(x))\n",
        "x=Flatten()(x)\n",
        "f= keras.layers.Concatenate(axis=1)([x,gender_embedding])\n",
        "print (K.int_shape(f)) \n",
        "#x = Dense(256, activation='relu')(x)\n",
        "prediction = Dense(1)(f)\n",
        "\n",
        "model = Model(inputs=[input,input_gender], outputs=prediction)\n",
        "for i,layer in enumerate(model.layers):\n",
        "    print (i,layer.name)\n",
        "\n",
        "Adam=tf.keras.optimizers.Adam(lr=0.0003,beta_1=0.9,beta_2=0.999)\n",
        "model.compile(optimizer=Adam, loss='mean_absolute_error', metrics=['MAE'])\n",
        "model.summary()\n",
        "\n",
        "checkpoint =keras.callbacks.ModelCheckpoint(filepath='weights_invrescam_atlas.{epoch:02d}-{val_loss:.2f}.hdf5',save_weights_only=True,period=30)\n",
        "#model.fit_generator(DataGen.flow([x_train,gender_train],y_train,batch_size=batch_size),steps_per_epoch=np.ceil(len(y_train)/batch_size),epochs=50,verbose=1,validation_data=([x_valid,gender_valid,agev],y_valid))\n",
        "history=model.fit([x_train,gender_train],y_train,batch_size=4,epochs=30,verbose=1,validation_data=([x_valid,gender_valid],y_valid),callbacks = [checkpoint])\n",
        "score = model.evaluate([x_test,gender_test], y_test, batch_size=batch_size)\n",
        "print('Test loss:', score[0])\n",
        "print('Test MAE:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sjNQTOJ0WBCk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ec701cd2-be4c-4893-bc09-db58d24a6b20"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00PfJMfMWD_Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b72f3b6-56c7-411d-b5cc-90729f96f0d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 9, 9, 1920)\n",
            "(None, 9, 9, 1920)\n",
            "(None, 2, 2, 1920)\n",
            "(None, 7696)\n",
            "0 input1\n",
            "1 densenet201\n",
            "2 tf.math.reduce_mean_2\n",
            "3 tf.math.reduce_max_2\n",
            "4 dense_13\n",
            "5 dense_15\n",
            "6 dense_14\n",
            "7 dense_16\n",
            "8 tf.__operators__.add_2\n",
            "9 tf.math.sigmoid_2\n",
            "10 tf.math.multiply_2\n",
            "11 max_pooling2d_10\n",
            "12 input2\n",
            "13 flatten_2\n",
            "14 dense_12\n",
            "15 concatenate_4\n",
            "16 dense_17\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input1 (InputLayer)            [(None, 300, 300, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " densenet201 (Functional)       (None, None, None,   18321984    ['input1[0][0]']                 \n",
            "                                1920)                                                             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_2 (TFOpLam  (None, 1, 1, 1920)  0           ['densenet201[0][0]']            \n",
            " bda)                                                                                             \n",
            "                                                                                                  \n",
            " tf.math.reduce_max_2 (TFOpLamb  (None, 1, 1, 1920)  0           ['densenet201[0][0]']            \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 1, 1, 240)    461040      ['tf.math.reduce_mean_2[0][0]']  \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 1, 1, 240)    461040      ['tf.math.reduce_max_2[0][0]']   \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 1, 1, 1920)   462720      ['dense_13[0][0]']               \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 1, 1, 1920)   462720      ['dense_15[0][0]']               \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 1, 1, 1920)  0           ['dense_14[0][0]',               \n",
            " mbda)                                                            'dense_16[0][0]']               \n",
            "                                                                                                  \n",
            " tf.math.sigmoid_2 (TFOpLambda)  (None, 1, 1, 1920)  0           ['tf.__operators__.add_2[0][0]'] \n",
            "                                                                                                  \n",
            " tf.math.multiply_2 (TFOpLambda  (None, 9, 9, 1920)  0           ['densenet201[0][0]',            \n",
            " )                                                                'tf.math.sigmoid_2[0][0]']      \n",
            "                                                                                                  \n",
            " max_pooling2d_10 (MaxPooling2D  (None, 2, 2, 1920)  0           ['tf.math.multiply_2[0][0]']     \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " input2 (InputLayer)            [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 7680)         0           ['max_pooling2d_10[0][0]']       \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 16)           32          ['input2[0][0]']                 \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 7696)         0           ['flatten_2[0][0]',              \n",
            "                                                                  'dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 1)            7697        ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 20,177,233\n",
            "Trainable params: 19,948,177\n",
            "Non-trainable params: 229,056\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "322/322 [==============================] - 85s 184ms/step - loss: 28.6336 - MAE: 28.6336 - val_loss: 29.7644 - val_MAE: 29.7644\n",
            "Epoch 2/30\n",
            "322/322 [==============================] - 49s 153ms/step - loss: 18.6639 - MAE: 18.6639 - val_loss: 20.2908 - val_MAE: 20.2908\n",
            "Epoch 3/30\n",
            "322/322 [==============================] - 49s 151ms/step - loss: 15.6864 - MAE: 15.6864 - val_loss: 20.8851 - val_MAE: 20.8851\n",
            "Epoch 4/30\n",
            "322/322 [==============================] - 49s 151ms/step - loss: 13.7632 - MAE: 13.7632 - val_loss: 21.5998 - val_MAE: 21.5998\n",
            "Epoch 5/30\n",
            "322/322 [==============================] - 49s 151ms/step - loss: 13.0595 - MAE: 13.0595 - val_loss: 16.3198 - val_MAE: 16.3198\n",
            "Epoch 6/30\n",
            "322/322 [==============================] - 49s 151ms/step - loss: 11.8278 - MAE: 11.8278 - val_loss: 14.3311 - val_MAE: 14.3311\n",
            "Epoch 7/30\n",
            "322/322 [==============================] - 49s 151ms/step - loss: 11.1901 - MAE: 11.1901 - val_loss: 21.3363 - val_MAE: 21.3363\n",
            "Epoch 8/30\n",
            "322/322 [==============================] - 49s 151ms/step - loss: 11.0826 - MAE: 11.0826 - val_loss: 20.9400 - val_MAE: 20.9400\n",
            "Epoch 9/30\n",
            "322/322 [==============================] - 49s 154ms/step - loss: 10.0831 - MAE: 10.0831 - val_loss: 15.2143 - val_MAE: 15.2143\n",
            "Epoch 10/30\n",
            "322/322 [==============================] - 49s 151ms/step - loss: 9.6444 - MAE: 9.6444 - val_loss: 19.2020 - val_MAE: 19.2020\n",
            "Epoch 11/30\n",
            "322/322 [==============================] - 49s 151ms/step - loss: 9.5411 - MAE: 9.5411 - val_loss: 14.2696 - val_MAE: 14.2696\n",
            "Epoch 12/30\n",
            "322/322 [==============================] - 49s 151ms/step - loss: 9.3614 - MAE: 9.3614 - val_loss: 16.3326 - val_MAE: 16.3326\n",
            "Epoch 13/30\n",
            "322/322 [==============================] - 49s 151ms/step - loss: 8.8085 - MAE: 8.8085 - val_loss: 14.9063 - val_MAE: 14.9063\n",
            "Epoch 14/30\n",
            "273/322 [========================>.....] - ETA: 7s - loss: 8.9489 - MAE: 8.9489"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.densenet import DenseNet201\n",
        "\n",
        "base_model = DenseNet201(weights='imagenet', include_top=False)\n",
        "input = Input(shape=(300,300,3),name='input1')\n",
        "input_gender = Input(shape=(1,),dtype='float32',name='input2')\n",
        "output = base_model(input)\n",
        "gender_embedding=Dense(16)(input_gender)\n",
        "print (K.int_shape(output))\n",
        "x=channel_attention(output)\n",
        "#print (K.int_shape(x))\n",
        "#x=spatial_attention(x)\n",
        "print (K.int_shape(x))\n",
        "x = keras.layers.MaxPooling2D(pool_size=(4,4))(x)\n",
        "print (K.int_shape(x))\n",
        "x=Flatten()(x)\n",
        "f= keras.layers.Concatenate(axis=1)([x,gender_embedding])\n",
        "print (K.int_shape(f)) \n",
        "#x = Dense(256, activation='relu')(x)\n",
        "prediction = Dense(1)(f)\n",
        "\n",
        "model = Model(inputs=[input,input_gender], outputs=prediction)\n",
        "for i,layer in enumerate(model.layers):\n",
        "    print (i,layer.name)\n",
        "\n",
        "Adam=tf.keras.optimizers.Adam(lr=0.0003,beta_1=0.9,beta_2=0.999)\n",
        "model.compile(optimizer=Adam, loss='mean_absolute_error', metrics=['MAE'])\n",
        "model.summary()\n",
        "\n",
        "checkpoint =keras.callbacks.ModelCheckpoint(filepath='weights_dens_atlas.{epoch:02d}-{val_loss:.2f}.hdf5',save_weights_only=True,period=30)\n",
        "#model.fit_generator(DataGen.flow([x_train,gender_train],y_train,batch_size=batch_size),steps_per_epoch=np.ceil(len(y_train)/batch_size),epochs=50,verbose=1,validation_data=([x_valid,gender_valid,agev],y_valid))\n",
        "history=model.fit([x_train,gender_train],y_train,batch_size=4,epochs=30,verbose=1,validation_data=([x_valid,gender_valid],y_valid),callbacks = [checkpoint])\n",
        "score = model.evaluate([x_test,gender_test], y_test, batch_size=batch_size)\n",
        "print('Test loss:', score[0])\n",
        "print('Test MAE:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37kzOG4VV9dx"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5MqCxxeWYrd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet import ResNet101\n",
        "\n",
        "base_model = ResNet101(weights='imagenet', include_top=False)\n",
        "input = Input(shape=(300,300,3),name='input1')\n",
        "input_gender = Input(shape=(1,),dtype='float32',name='input2')\n",
        "output = base_model(input)\n",
        "gender_embedding=Dense(16)(input_gender)\n",
        "print (K.int_shape(output))\n",
        "x=channel_attention(output)\n",
        "#print (K.int_shape(x))\n",
        "#x=spatial_attention(x)\n",
        "print (K.int_shape(x))\n",
        "x = keras.layers.MaxPooling2D(pool_size=(4,4))(x)\n",
        "print (K.int_shape(x))\n",
        "x=Flatten()(x)\n",
        "f= keras.layers.Concatenate(axis=1)([x,gender_embedding])\n",
        "print (K.int_shape(f)) \n",
        "#x = Dense(256, activation='relu')(x)\n",
        "prediction = Dense(1)(f)\n",
        "\n",
        "model = Model(inputs=[input,input_gender], outputs=prediction)\n",
        "for i,layer in enumerate(model.layers):\n",
        "    print (i,layer.name)\n",
        "\n",
        "Adam=tf.keras.optimizers.Adam(lr=0.0003,beta_1=0.9,beta_2=0.999)\n",
        "model.compile(optimizer=Adam, loss='mean_absolute_error', metrics=['MAE'])\n",
        "model.summary()\n",
        "\n",
        "checkpoint =keras.callbacks.ModelCheckpoint(filepath='weights_res101cam_atlas.{epoch:02d}-{val_loss:.2f}.hdf5',save_weights_only=True,period=30)\n",
        "#model.fit_generator(DataGen.flow([x_train,gender_train],y_train,batch_size=batch_size),steps_per_epoch=np.ceil(len(y_train)/batch_size),epochs=50,verbose=1,validation_data=([x_valid,gender_valid,agev],y_valid))\n",
        "history=model.fit([x_train,gender_train],y_train,batch_size=4,epochs=30,verbose=1,validation_data=([x_valid,gender_valid],y_valid),callbacks = [checkpoint])\n",
        "score = model.evaluate([x_test,gender_test], y_test, batch_size=batch_size)\n",
        "print('Test loss:', score[0])\n",
        "print('Test MAE:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2jgDTOkWWrn"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HiiodtWDVyek"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}